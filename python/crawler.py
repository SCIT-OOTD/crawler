from playwright.sync_api import sync_playwright
import sys
import time
import re

# 1. í•œê¸€ ê¹¨ì§ ë°©ì§€
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

def parse_korean_number(text):
    if not text: return 0
    text = str(text).strip()
    multiplier = 1
    
    if 'ë§Œ' in text:
        multiplier = 10000
        text = text.replace('ë§Œ', '')
    elif 'ì²œ' in text:
        multiplier = 1000
        text = text.replace('ì²œ', '')
    
    clean_num = re.sub(r"[^0-9.]", "", text)
    if clean_num:
        try:
            return int(float(clean_num) * multiplier)
        except:
            return 0
    return 0

def scrape_musinsa():
    total_results = []
    
    # ì•„ìš°í„°ê¹Œì§€ í¬í•¨ëœ ì¹´í…Œê³ ë¦¬
    CATEGORY_URLS = {
        "ìƒì˜": "https://www.musinsa.com/main/musinsa/ranking?gf=A&storeCode=musinsa&sectionId=200&categoryCode=001000",
        "í•˜ì˜": "https://www.musinsa.com/main/musinsa/ranking?gf=A&storeCode=musinsa&sectionId=200&categoryCode=003000",
        "ì‹ ë°œ": "https://www.musinsa.com/main/musinsa/ranking?gf=A&storeCode=musinsa&sectionId=200&categoryCode=005000",
        "ì•„ìš°í„°": "https://www.musinsa.com/main/musinsa/ranking?gf=A&storeCode=musinsa&sectionId=200&categoryCode=002000"
    }

    print(">> [ë¬´ì‹ ì‚¬] í†µí•© í¬ë¡¤ë§ ì‹œì‘ (ìŠ¤í¬ë¡¤ ë³´ì •)...", flush=True)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False) 
        page = browser.new_page()

        for cat_name, cat_url in CATEGORY_URLS.items():
            print(f"\n>> ğŸš€ [{cat_name}] ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹œì‘...", flush=True)
            
            page.goto(cat_url, timeout=60000)
            time.sleep(2)

            # ğŸ› ï¸ [ì¶”ê°€ë¨] ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸°! (ìƒí’ˆì´ ë‹¤ ë¡œë”©ë˜ë„ë¡)
            for _ in range(5): # 5ë²ˆ ì •ë„ íˆ­íˆ­ ë‚´ë¦½ë‹ˆë‹¤
                page.keyboard.press("PageDown")
                time.sleep(0.5)
            
            # ë§¨ ë°‘ìœ¼ë¡œ í•œë²ˆ ë” í™• ë‚´ë¦¬ê¸°
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)

            # 2. ë§í¬ ìˆ˜ì§‘ (ì´ì œ ë¡œë”©ëœ ê²Œ ë§ìœ¼ë‹ˆ ë„‰ë„‰í•˜ê²Œ 80ê°œ ê¸ì–´ì˜´)
            items_data = page.evaluate("""() => {
                const data = [];
                const links = Array.from(document.querySelectorAll("a"));
                const productLinks = links.filter(a => 
                    (a.href.includes('/goods/') || a.href.includes('/products/')) && 
                    !a.href.includes('reviews')
                );
                // ì¤‘ë³µ ì œê±° ë° í•„í„°ë§ì„ ìœ„í•´ ë„‰ë„‰íˆ 80ê°œ ê°€ì ¸ì˜´
                productLinks.slice(0, 80).forEach(a => {
                   data.push({ href: a.href }); 
                });
                return data;
            }""")

            target_items = []
            seen = set()
            for item in items_data:
                url = item['href'].split('?')[0]
                if url not in seen:
                    seen.add(url)
                    target_items.append(item)
                if len(target_items) >= 30: break # ì—¬ê¸°ì„œ 30ê°œ ëŠê¸°
            
            print(f">> [{cat_name}] í™•ë³´ëœ ë§í¬: {len(target_items)}ê°œ", flush=True)

            # 3. ìƒì„¸ í˜ì´ì§€ ìˆœíšŒ
            for idx, item in enumerate(target_items):
                try:
                    page.goto(item['href'], timeout=60000)
                    time.sleep(1.2) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

                    extracted = page.evaluate("""() => {
                        const getMeta = (p) => document.querySelector(`meta[property="${p}"]`)?.content || "";
                        
                        let rating = "0";
                        let reviewCountTxt = "0";
                        const reviewBox = document.querySelector("div[data-button-id='review']");
                        if (reviewBox) {
                            const spans = reviewBox.querySelectorAll("span");
                            if (spans.length > 0) rating = spans[0].innerText;
                            if (spans.length > 1) reviewCountTxt = spans[1].innerText;
                        }

                        let likes = "0";
                        const likeIcon = document.querySelector("svg[data-mds='IcBoldLike']");
                        if (likeIcon) {
                            const container = likeIcon.closest("div");
                            if (container) { likes = container.innerText; }
                        }

                        // ì„œë¸Œ ì´ë¯¸ì§€ (ì‚¬ìš©ì ìš”ì²­ íƒœê·¸ ë°˜ì˜)
                        let subImgs = [];
                        const bullets = document.querySelectorAll("div[class*='Pagination__Bullet'] img");
                        bullets.forEach(img => { if (img.src) subImgs.push(img.src); });
                        
                        // êµ¬í˜• í˜ì´ì§€ ëŒ€ë¹„
                        if (subImgs.length === 0) {
                            const oldThumbs = document.querySelectorAll('.product_thumb img');
                            oldThumbs.forEach(img => subImgs.push(img.src));
                        }
                        const subImgString = subImgs.join(',');

                        let viewCount = "0";
                        const stats = document.querySelectorAll("#page_view"); 
                        if (stats.length > 0) viewCount = stats[0].innerText;
                        
                        return {
                            title: getMeta('og:title'),
                            brand: getMeta('product:brand'),
                            img: getMeta('og:image'),
                            price: getMeta('product:price:amount'),
                            rating: rating,
                            reviews: reviewCountTxt,
                            likes: likes,
                            sub_imgs: subImgString,
                            view_count: viewCount
                        };
                    }""")

                    final_rating = 0.0
                    try: final_rating = float(extracted['rating'])
                    except: pass

                    final_likes = parse_korean_number(extracted['likes'])
                    final_reviews = parse_korean_number(extracted['reviews'])
                    final_views = parse_korean_number(extracted['view_count'])
                    price_int = int(extracted['price']) if extracted['price'] else 0
                    brand_name = extracted['brand'] if extracted['brand'] else "ë¬´ì‹ ì‚¬"

                    data = {
                        "ranking": idx + 1,
                        "brand": brand_name,
                        "title": extracted['title'],
                        "price": price_int,
                        "img_url": extracted['img'],
                        "category": cat_name, 
                        "like_count": final_likes,
                        "rating": final_rating,
                        "review_count": final_reviews,
                        "sub_img": extracted['sub_imgs'], 
                        "view_count": final_views
                    }
                    
                    total_results.append(data)
                    print(f"[{cat_name} {idx+1}] {extracted['title'][:5]}... ì™„ë£Œ", flush=True)

                except Exception as e:
                    # ì‹¤íŒ¨í•´ë„ ë©ˆì¶”ì§€ ì•Šê³  ë‹¤ìŒ ê±¸ë¡œ ë„˜ì–´ê°
                    print(f"[{cat_name} {idx+1}] âŒ ì‹¤íŒ¨(Skip): {e}", flush=True)
            
            time.sleep(2)

        browser.close()

    return total_results

if __name__ == "__main__":
    data = scrape_musinsa()
    print(f"ì´ í¬ë¡¤ë§ ê²°ê³¼: {len(data)}ê°œ")