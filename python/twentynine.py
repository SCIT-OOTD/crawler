import sys
import io
import json
import os
import time
import random

# ‚òÖ Ï§ëÏöî: ÏùºÎ∞ò selenium ÎåÄÏã† undetected_chromedriver ÏÇ¨Ïö©
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# ÌïúÍ∏Ä Íπ®Ïßê Î∞©ÏßÄ
sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')

def crawl_pc_stealth():
    # 29CM ÎÇ®ÏÑ± ÏùòÎ•ò (PC Ï£ºÏÜå)
    list_url = "https://www.29cm.co.kr/store/category/list?categoryLargeCode=272100100&categoryMediumCode=272103100"

    print(">> [PC Ïä§ÌÖîÏä§ Î™®Îìú] Îç∞Ïä§ÌÅ¨ÌÉë ÌôòÍ≤ΩÏóêÏÑú Ï∞®Îã®ÏùÑ Ïö∞ÌöåÌï©ÎãàÎã§...")

    data_list = []
    driver = None

    try:
        # 1. Î≥¥Ïïà Ïö∞Ìöå Î∏åÎùºÏö∞Ï†Ä ÏÑ§Ï†ï (Î™®Î∞îÏùº ÏÑ§Ï†ï X)
        options = uc.ChromeOptions()
        # options.add_argument('--headless') # ÌôîÎ©¥ Î≥¥Í≥† Ïã∂ÏúºÎ©¥ Ï£ºÏÑù Ïú†ÏßÄ
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')

        # Î∏åÎùºÏö∞Ï†Ä Ïã§Ìñâ (Î≤ÑÏ†Ñ ÏûêÎèô Îß§Ïπ≠)
        driver = uc.Chrome(options=options, version_main=None)

        # ‚òÖ ÌôîÎ©¥ÏùÑ ÎÑìÍ≤å Ïç®Ïïº PC Î†àÏù¥ÏïÑÏõÉÏù¥ ÎÇòÏò¥
        driver.set_window_size(1920, 1080)

        print(f">> PC ÌéòÏù¥ÏßÄ Ï†ëÏÜç: {list_url}")
        driver.get(list_url)

        # 2. Î°úÎî© ÎåÄÍ∏∞ (PCÎäî Î°úÎî©Ïù¥ Ï¢Ä Îçî Í±∏Î¶¥ Ïàò ÏûàÏùå)
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/product/']"))
            )
            print(">> ÏÉÅÌíà Î™©Î°ù Î°úÎî© ÏÑ±Í≥µ!")
        except:
            print("üö® [Ï∞®Îã® Í∞êÏßÄ] PC Î≤ÑÏ†Ñ Ï†ëÏÜçÏù¥ Ï∞®Îã®ÎêòÏóàÏäµÎãàÎã§. IP Î≥ÄÍ≤Ω(Ìï´Ïä§Ìåü)Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
            driver.save_screenshot("pc_block_error.png")
            return []

        # Ïä§ÌÅ¨Î°§ ÎÇ¥Î†§ÏÑú Îç∞Ïù¥ÌÑ∞ Î°úÎî©
        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # 3. ÏÉÅÌíà ÎßÅÌÅ¨ ÏàòÏßë
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        anchors = soup.find_all('a', href=True)

        product_urls = []
        seen_ids = set()

        for a in anchors:
            href = a['href']
            if '/product/' in href:
                # https://product.29cm.co.kr/... ÌòïÌÉú Ï≤òÎ¶¨
                if href.startswith('http'): full_url = href
                else: full_url = "https://www.29cm.co.kr" + href

                p_id = full_url.split('/')[-1].split('?')[0]
                if p_id.isdigit() and p_id not in seen_ids:
                    product_urls.append(full_url)
                    seen_ids.add(p_id)

        # ÏÉÅÏúÑ 6Í∞úÎßå ÌÖåÏä§Ìä∏ (ÏïÑÎ©îÏä§ Ìè¨Ìï® ÌôïÏù∏Ïö©)
        target_urls = product_urls[:6]
        print(f">> ÏàòÏßë ÎåÄÏÉÅ: {len(target_urls)}Í∞ú")

        # 4. ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ ÏàúÌöå
        for i, p_url in enumerate(target_urls):
            try:
                print(f" -> [{i+1}] ÏÉÅÏÑ∏ ÏßÑÏûÖ: {p_url}")
                driver.get(p_url)

                # Ïù¥ÎØ∏ÏßÄ Ïä¨ÎùºÏù¥Îçî Î°úÎî© ÎåÄÍ∏∞ (PCÎäî Ïù¥ÎØ∏ÏßÄÍ∞Ä ÌÅº)
                time.sleep(random.uniform(2.5, 4))

                detail_soup = BeautifulSoup(driver.page_source, 'html.parser')

                # (1) Î∏åÎûúÎìú/Ï†úÎ™©
                brand = "29CM"
                title = "Unknown"
                og_title = detail_soup.find("meta", property="og:title")
                if og_title:
                    content = og_title["content"]
                    if ']' in content:
                        parts = content.split(']')
                        brand = parts[0].replace('[', '').strip()
                        title = parts[1].strip()
                    else:
                        title = content

                # (2) Í∞ÄÍ≤©
                price = 0
                text = detail_soup.get_text()
                import re
                matches = re.findall(r'([\d,]+)Ïõê', text)
                for m in matches:
                    p = int(m.replace(',', ''))
                    if p > 1000:
                        price = p
                        break

                # (3) Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ¶¨ (PC Î≤ÑÏ†Ñ Î°úÏßÅ)
                valid_imgs = []
                imgs = detail_soup.find_all('img')

                for img in imgs:
                    src = img.get('src', '')
                    if not src: continue
                    if not src.startswith('http'): src = "https:" + src

                    # Ïç∏ÎÑ§Ïùº Ï†úÏô∏, Î©îÏù∏ Ïù¥ÎØ∏ÏßÄÎßå
                    if ('/item/' in src or '/product/' in src) and '.svg' not in src:
                        if '60x60' not in src and '50x50' not in src:
                            if src not in valid_imgs:
                                valid_imgs.append(src)

                model_img = ""
                cloth_img = ""

                # [ÌïÑÏäπ Î°úÏßÅ] 0Î≤à: Î™®Îç∏, 1Î≤à: Ïò∑
                if len(valid_imgs) >= 2:
                    model_img = valid_imgs[0]
                    cloth_img = valid_imgs[1]
                elif len(valid_imgs) == 1:
                    model_img = valid_imgs[0]
                    cloth_img = valid_imgs[0]

                print(f"    - Î™®Îç∏: {model_img[-20:]}")
                print(f"    - Ïò∑  : {cloth_img[-20:]}")

                obj = {
                    "product_no": p_url.split('/')[-1].split('?')[0],
                    "source": "29CM_PC",
                    "brand": brand,
                    "title": title,
                    "price": price,
                    "cloth_img": cloth_img,
                    "model_img": model_img
                }
                data_list.append(obj)

            except Exception as e:
                print(f"Error: {e}")
                continue

    except Exception as e:
        print(f"Fatal Error: {e}")
    finally:
        if driver: driver.quit()

    return data_list

if __name__ == "__main__":
    result = crawl_pc_stealth()

    current_dir = os.path.dirname(os.path.abspath(__file__))
    save_path = os.path.join(current_dir, 'twentynine_ai_data.json')

    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=4)